"""
SLM NPU ç¨ç«‹å¾Œç«¯æœå‹™å™¨
å°ˆé–€ç”¨æ–¼SLMå­—å‹ç”Ÿæˆï¼Œä¸ä¾è³´æ–¼ç¾æœ‰çš„å¾Œç«¯
"""

from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
import io
import base64
import sys
import os
import time
from typing import Optional
import uvicorn

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from slm_generator import create_slm_generator, SLMFontGenerator

# å‰µå»ºç¨ç«‹çš„FastAPIæ‡‰ç”¨
app = FastAPI(
    title="SLM NPU Font Generator",
    description="ç¨ç«‹çš„SLM NPUå­—å‹ç”Ÿæˆæœå‹™",
    version="1.0.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # é–‹ç™¼æ™‚å…è¨±å…¨éƒ¨ä¾†æº
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€SLMç”Ÿæˆå™¨å¯¦ä¾‹
slm_generator: Optional[SLMFontGenerator] = None

def get_slm_generator() -> SLMFontGenerator:
    """ç²å–æˆ–å‰µå»ºSLMç”Ÿæˆå™¨å¯¦ä¾‹"""
    global slm_generator
    if slm_generator is None:
        # æª¢æŸ¥æ˜¯å¦æœ‰çœŸå¯¦çš„SLMæ¨¡å‹
        model_path = "./models/slm_font_model.onnx"
        use_npu = os.path.exists(model_path)
        
        slm_generator = create_slm_generator(
            model_path=model_path if use_npu else None,
            use_npu=use_npu
        )
        
        print(f"ğŸ”§ SLMç”Ÿæˆå™¨å·²åˆå§‹åŒ–: {'NPUæ¨¡å¼' if use_npu else 'æ¨¡æ“¬æ¨¡å¼'}")
    
    return slm_generator

def _generate_slm_response(characters: str, context: str) -> str:
    """ç”ŸæˆSLMå›æ‡‰ï¼ˆä½¿ç”¨çœŸæ­£çš„èªè¨€æ¨¡å‹ï¼‰"""
    try:
        # å˜—è©¦ä½¿ç”¨çœŸæ­£çš„èªè¨€æ¨¡å‹
        try:
            from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
            import torch
            
            # ä¸»è¦èªè¨€æ¨¡å‹
            primary_model_name = "microsoft/DialoGPT-medium"
            # å‚™ç”¨ phi æ¨¡å‹
            backup_model_name = "microsoft/phi-2"  # æˆ–å…¶ä»– phi é–‹é ­çš„æ¨¡å‹
            
            print(f"[SLM] ğŸ¤– å˜—è©¦è¼‰å…¥ä¸»è¦èªè¨€æ¨¡å‹: {primary_model_name}")
            
            # å˜—è©¦è¼‰å…¥ä¸»è¦æ¨¡å‹
            try:
                tokenizer = AutoTokenizer.from_pretrained(primary_model_name)
                model = AutoModelForCausalLM.from_pretrained(primary_model_name)
                current_model = primary_model_name
                print(f"[SLM] âœ… ä¸»è¦æ¨¡å‹è¼‰å…¥æˆåŠŸ: {primary_model_name}")
            except Exception as primary_error:
                print(f"[SLM] âš ï¸ ä¸»è¦æ¨¡å‹è¼‰å…¥å¤±æ•—: {primary_error}")
                print(f"[SLM] ğŸ”„ å˜—è©¦è¼‰å…¥å‚™ç”¨ phi æ¨¡å‹: {backup_model_name}")
                
                try:
                    tokenizer = AutoTokenizer.from_pretrained(backup_model_name)
                    model = AutoModelForCausalLM.from_pretrained(backup_model_name)
                    current_model = backup_model_name
                    print(f"[SLM] âœ… å‚™ç”¨ phi æ¨¡å‹è¼‰å…¥æˆåŠŸ: {backup_model_name}")
                except Exception as backup_error:
                    print(f"[SLM] âŒ å‚™ç”¨ phi æ¨¡å‹ä¹Ÿè¼‰å…¥å¤±æ•—: {backup_error}")
                    raise Exception(f"æ‰€æœ‰èªè¨€æ¨¡å‹éƒ½è¼‰å…¥å¤±æ•—: {primary_error}, {backup_error}")
            
            # è¨­ç½®pad_token - ä¿®å¾©è­¦å‘Š
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                print(f"[SLM] ğŸ”§ è¨­ç½®pad_token: {tokenizer.pad_token}")
            
            # æ§‹å»ºæ›´è±å¯Œçš„æç¤ºè©
            if context and context.strip():
                prompt = f"""User: è«‹è©³ç´°åˆ†æä¸¦å›ç­”é—œæ–¼ã€Œ{characters}ã€çš„å•é¡Œã€‚ä¸Šä¸‹æ–‡ï¼š{context}

è«‹æä¾›ï¼š
1. è©³ç´°çš„æŠ€è¡“åˆ†æ
2. å¯¦ç”¨çš„å»ºè­°å’Œè§£æ±ºæ–¹æ¡ˆ
3. ç›¸é—œçš„æŠ€è¡“èƒŒæ™¯çŸ¥è­˜
4. å…·é«”çš„å¯¦æ–½æ­¥é©Ÿ

Assistant:"""
            else:
                prompt = f"""User: è«‹è©³ç´°åˆ†æä¸¦å›ç­”é—œæ–¼ã€Œ{characters}ã€çš„å•é¡Œã€‚

è«‹æä¾›ï¼š
1. è©³ç´°çš„æŠ€è¡“åˆ†æ
2. å¯¦ç”¨çš„å»ºè­°å’Œè§£æ±ºæ–¹æ¡ˆ
3. ç›¸é—œçš„æŠ€è¡“èƒŒæ™¯çŸ¥è­˜
4. å…·é«”çš„å¯¦æ–½æ­¥é©Ÿ

Assistant:"""
            
            print(f"[SLM] ğŸ“ ä½¿ç”¨æç¤ºè©: {prompt}")
            
            # ç·¨ç¢¼è¼¸å…¥
            inputs = tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=100)
            
            # å¤šé‡ç”Ÿæˆç­–ç•¥ - å¤§å¹…é™ä½ç©ºå›æ‡‰æ©Ÿç‡
            generated_responses = []
            
            # ç­–ç•¥1: æ¨™æº–ç”Ÿæˆ
            try:
                with torch.no_grad():
                    outputs = model.generate(
                        inputs, 
                        max_new_tokens=200,  # å¾80å¢åŠ åˆ°200
                        num_return_sequences=1,
                        temperature=0.8,  # ç¨å¾®é™ä½æº«åº¦ä»¥ç²å¾—æ›´é€£è²«çš„å›æ‡‰
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id,
                        repetition_penalty=1.1,  # é™ä½é‡è¤‡æ‡²ç½°
                        no_repeat_ngram_size=3,  # å¢åŠ n-gramå¤§å°
                        top_k=50,  # æ·»åŠ top_kåƒæ•¸
                        top_p=0.9,  # æ·»åŠ top_påƒæ•¸
                        # ç§»é™¤ length_penalty ä»¥é¿å…è­¦å‘Šï¼Œå› ç‚ºæˆ‘å€‘æ²’æœ‰ä½¿ç”¨ beam search
                    )
                
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                generated_response = response[len(prompt):].strip()
                
                if generated_response and len(generated_response) > 10:
                    generated_responses.append(("æ¨™æº–ç”Ÿæˆ", generated_response))
                    print(f"[SLM] âœ… æ¨™æº–ç”ŸæˆæˆåŠŸï¼Œå›æ‡‰é•·åº¦: {len(generated_response)}")
                
            except Exception as e:
                print(f"[SLM] âš ï¸ æ¨™æº–ç”Ÿæˆå¤±æ•—: {e}")
            
            # ç­–ç•¥2: é«˜æº«åº¦ç”Ÿæˆï¼ˆå¢åŠ å¤šæ¨£æ€§ï¼‰
            if not generated_responses:
                try:
                    with torch.no_grad():
                        outputs = model.generate(
                            inputs, 
                            max_new_tokens=150,
                            num_return_sequences=1,
                            temperature=1.2,  # é«˜æº«åº¦å¢åŠ å¤šæ¨£æ€§
                            do_sample=True,
                            pad_token_id=tokenizer.pad_token_id,
                            eos_token_id=tokenizer.eos_token_id,
                            repetition_penalty=1.0,  # é™ä½é‡è¤‡æ‡²ç½°
                            no_repeat_ngram_size=2,
                            top_k=100,
                            top_p=0.95
                        )
                    
                    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    generated_response = response[len(prompt):].strip()
                    
                    if generated_response and len(generated_response) > 10:
                        generated_responses.append(("é«˜æº«åº¦ç”Ÿæˆ", generated_response))
                        print(f"[SLM] âœ… é«˜æº«åº¦ç”ŸæˆæˆåŠŸï¼Œå›æ‡‰é•·åº¦: {len(generated_response)}")
                        
                except Exception as e:
                    print(f"[SLM] âš ï¸ é«˜æº«åº¦ç”Ÿæˆå¤±æ•—: {e}")
            
            # ç­–ç•¥3: è²ªå¿ƒæœç´¢ï¼ˆç¢ºä¿æœ‰è¼¸å‡ºï¼‰
            if not generated_responses:
                try:
                    with torch.no_grad():
                        outputs = model.generate(
                            inputs, 
                            max_new_tokens=100,
                            num_return_sequences=1,
                            do_sample=False,  # è²ªå¿ƒæœç´¢
                            pad_token_id=tokenizer.pad_token_id,
                            eos_token_id=tokenizer.eos_token_id
                        )
                    
                    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    generated_response = response[len(prompt):].strip()
                    
                    if generated_response and len(generated_response) > 10:
                        generated_responses.append(("è²ªå¿ƒæœç´¢", generated_response))
                        print(f"[SLM] âœ… è²ªå¿ƒæœç´¢æˆåŠŸï¼Œå›æ‡‰é•·åº¦: {len(generated_response)}")
                        
                except Exception as e:
                    print(f"[SLM] âš ï¸ è²ªå¿ƒæœç´¢å¤±æ•—: {e}")
            
            # ç­–ç•¥4: ç°¡åŒ–æç¤ºè©é‡è©¦
            if not generated_responses:
                try:
                    simple_prompt = f"User: è«‹å›ç­”é—œæ–¼ã€Œ{characters}ã€çš„å•é¡Œã€‚\nAssistant:"
                    simple_inputs = tokenizer.encode(simple_prompt, return_tensors="pt", truncation=True, max_length=50)
                    
                    with torch.no_grad():
                        outputs = model.generate(
                            simple_inputs, 
                            max_new_tokens=80,
                            num_return_sequences=1,
                            temperature=0.9,
                            do_sample=True,
                            pad_token_id=tokenizer.pad_token_id,
                            eos_token_id=tokenizer.eos_token_id
                        )
                    
                    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    generated_response = response[len(simple_prompt):].strip()
                    
                    if generated_response and len(generated_response) > 10:
                        generated_responses.append(("ç°¡åŒ–æç¤ºè©", generated_response))
                        print(f"[SLM] âœ… ç°¡åŒ–æç¤ºè©ç”ŸæˆæˆåŠŸï¼Œå›æ‡‰é•·åº¦: {len(generated_response)}")
                        
                except Exception as e:
                    print(f"[SLM] âš ï¸ ç°¡åŒ–æç¤ºè©ç”Ÿæˆå¤±æ•—: {e}")
            
            # é¸æ“‡æœ€ä½³å›æ‡‰
            if generated_responses:
                # é¸æ“‡æœ€é•·çš„å›æ‡‰
                best_response = max(generated_responses, key=lambda x: len(x[1]))
                print(f"[SLM] ğŸ¯ é¸æ“‡æœ€ä½³å›æ‡‰: {best_response[0]}, é•·åº¦: {len(best_response[1])}")
                
                # æª¢æŸ¥å›æ‡‰æ˜¯å¦ç‚ºç©ºæˆ–éçŸ­
                if not best_response[1] or len(best_response[1].strip()) < 10:
                    print(f"[SLM] âš ï¸ ä¸»è¦æ¨¡å‹å›æ‡‰ç‚ºç©ºï¼Œå˜—è©¦åˆ‡æ›åˆ° phi æ¨¡å‹")
                    return _try_phi_model_fallback(characters, context)
                
                return best_response[1]
            else:
                print(f"[SLM] âš ï¸ æ‰€æœ‰èªè¨€æ¨¡å‹ç­–ç•¥éƒ½å¤±æ•—ï¼Œå˜—è©¦ phi æ¨¡å‹å‚™ç”¨")
                return _try_phi_model_fallback(characters, context)
                
        except Exception as model_error:
            print(f"[SLM] âš ï¸ èªè¨€æ¨¡å‹è¼‰å…¥å¤±æ•—: {model_error}")
            print(f"[SLM] ğŸ”„ å˜—è©¦ phi æ¨¡å‹å‚™ç”¨")
            
            # å˜—è©¦ phi æ¨¡å‹å‚™ç”¨
            phi_response = _try_phi_model_fallback(characters, context)
            if phi_response:
                return phi_response
            
            # å¦‚æœ phi æ¨¡å‹ä¹Ÿå¤±æ•—ï¼Œä½¿ç”¨æ™ºèƒ½æ¨¡æ¿
            print(f"[SLM] ğŸ”„ åˆ‡æ›åˆ°æ™ºèƒ½æ¨¡æ¿æ¨¡å¼")
            return _generate_smart_template_response(characters, context)
            
    except Exception as e:
        print(f"[SLM] âŒ ç”ŸæˆSLMå›æ‡‰æ™‚å‡ºéŒ¯: {e}")
        return f"SLMå›æ‡‰ç”Ÿæˆå¤±æ•—: {str(e)}"

def _try_phi_model_fallback(characters: str, context: str) -> str:
    """å˜—è©¦ä½¿ç”¨ phi æ¨¡å‹ä½œç‚ºå‚™ç”¨"""
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        # å˜—è©¦ä¸åŒçš„ phi æ¨¡å‹
        phi_models = [
            "microsoft/Phi-3-mini",  # ä¸»è¦çš„ phi æ¨¡å‹
            "microsoft/phi-2",        # å‚™ç”¨ phi-2
            "microsoft/phi-1_5"       # å‚™ç”¨ phi-1.5
        ]
        
        for phi_model in phi_models:
            try:
                print(f"[SLM] ğŸ”„ å˜—è©¦è¼‰å…¥ phi æ¨¡å‹: {phi_model}")
                
                tokenizer = AutoTokenizer.from_pretrained(phi_model)
                model = AutoModelForCausalLM.from_pretrained(phi_model)
                
                # è¨­ç½® pad_token
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                # æ§‹å»ºé©åˆ phi æ¨¡å‹çš„æç¤ºè©
                if context and context.strip():
                    prompt = f"User: è«‹åˆ†æã€Œ{characters}ã€ä¸¦æä¾›å»ºè­°ã€‚ä¸Šä¸‹æ–‡ï¼š{context}\n\nAssistant:"
                else:
                    prompt = f"User: è«‹åˆ†æã€Œ{characters}ã€ä¸¦æä¾›å»ºè­°ã€‚\n\nAssistant:"
                
                print(f"[SLM] ğŸ“ Phi æ¨¡å‹æç¤ºè©: {prompt}")
                
                # ç·¨ç¢¼è¼¸å…¥
                inputs = tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=100)
                
                # ä½¿ç”¨ phi æ¨¡å‹ç”Ÿæˆ
                with torch.no_grad():
                    outputs = model.generate(
                        inputs,
                        max_new_tokens=150,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id,
                        repetition_penalty=1.1,
                        top_k=50,
                        top_p=0.9
                    )
                
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                generated_response = response[len(prompt):].strip()
                
                if generated_response and len(generated_response) > 10:
                    print(f"[SLM] âœ… Phi æ¨¡å‹ç”ŸæˆæˆåŠŸ: {phi_model}, å›æ‡‰é•·åº¦: {len(generated_response)}")
                    return generated_response
                else:
                    print(f"[SLM] âš ï¸ Phi æ¨¡å‹å›æ‡‰ç‚ºç©º: {phi_model}")
                    
            except Exception as phi_error:
                print(f"[SLM] âš ï¸ Phi æ¨¡å‹ {phi_model} è¼‰å…¥å¤±æ•—: {phi_error}")
                continue
        
        print(f"[SLM] âŒ æ‰€æœ‰ phi æ¨¡å‹éƒ½å¤±æ•—")
        return None
        
    except Exception as e:
        print(f"[SLM] âŒ Phi æ¨¡å‹å‚™ç”¨å¤±æ•—: {e}")
        return None

def _generate_smart_template_response(characters: str, context: str) -> str:
    """è¬ç”¨ç¥ç¦æ–‡å­—å›æ‡‰ï¼ˆç•¶èªè¨€æ¨¡å‹ä¸å¯ç”¨æ™‚ï¼‰"""
    try:
        import random
        import time
        
        # ç²å–ç•¶å‰æ™‚é–“
        current_time = time.strftime("%Y-%m-%d %H:%M:%S")
        
        # è¬ç”¨ç¥ç¦èªå¥é›†åˆ
        universal_blessings = [
            "ğŸŒŸ May your journey be filled with joy, success, and endless possibilities! ğŸŒŸ",
            "âœ¨ Wishing you strength, wisdom, and happiness in all your endeavors! âœ¨",
            "ğŸ‰ May every step you take lead you closer to your dreams and aspirations! ğŸ‰",
            "ğŸ’« Sending you positive energy and warm wishes for a wonderful day ahead! ğŸ’«",
            "ğŸŒˆ May your path be illuminated with hope, love, and beautiful moments! ğŸŒˆ",
            "ğŸš€ Here's to new beginnings, exciting adventures, and amazing achievements! ğŸš€",
            "ğŸŠ May your heart be filled with peace, your mind with clarity, and your soul with joy! ğŸŠ",
            "â­ Wishing you courage to face challenges and wisdom to overcome obstacles! â­",
            "ğŸŒº May your life be a beautiful garden of happiness, love, and success! ğŸŒº",
            "ğŸ¯ Sending you blessings for health, wealth, and all the good things in life! ğŸ¯"
        ]
        
        # æ ¹æ“šå­—å…ƒå…§å®¹é¸æ“‡åˆé©çš„ç¥ç¦èª
        char_count = len(characters)
        has_chinese = any('\u4e00' <= char <= '\u9fff' for char in characters)
        has_english = any(char.isascii() and char.isalpha() for char in characters)
        has_numbers = any(char.isdigit() for char in characters)
        
        # é¸æ“‡ç¥ç¦èª
        if has_chinese:
            selected_blessing = random.choice([
                "ğŸŒŸ é¡˜ä½ çš„å­—å‹å‰µä½œä¹‹è·¯å……æ»¿éˆæ„Ÿèˆ‡ç¾å¥½ï¼ğŸŒŸ",
                "âœ¨ é¡˜æ¯å€‹å­—å…ƒéƒ½æ‰¿è¼‰è‘—ä½ çš„å¤¢æƒ³èˆ‡å¸Œæœ›ï¼âœ¨",
                "ğŸ¨ é¡˜ä½ çš„è—è¡“å¤©è³¦ç¶»æ”¾å‡ºæœ€ç¾éº—çš„å…‰èŠ’ï¼ğŸ¨"
            ])
        elif has_english:
            selected_blessing = random.choice([
                "ğŸŒŸ May your typography journey be filled with creativity and beauty! ğŸŒŸ",
                "âœ¨ May each character carry your dreams and hopes! âœ¨",
                "ğŸ¨ May your artistic talent shine with the most beautiful light! ğŸ¨"
            ])
        elif has_numbers:
            selected_blessing = random.choice([
                "ğŸ”¢ May your numerical creations bring order and harmony! ğŸ”¢",
                "ğŸ“Š May your data-driven designs inspire and enlighten! ğŸ“Š",
                "âš¡ May your digital innovations spark creativity! âš¡"
            ])
        else:
            selected_blessing = random.choice(universal_blessings)
        
        # æ§‹å»ºç°¡çŸ­çš„è¬ç”¨ç¥ç¦å›æ‡‰
        response = f"""ğŸ‰ **Universal Blessing** ğŸ‰

{selected_blessing}

ğŸ’« May your day be filled with joy and creativity! ğŸ’«

---
ğŸŒŸ *Generated with love and care* ğŸŒŸ"""
        
        return response
        
    except Exception as e:
        print(f"[SLM] âŒ è¬ç”¨ç¥ç¦ç”Ÿæˆå¤±æ•—: {e}")
        return f"""ğŸ‰ **Universal Blessing** ğŸ‰

ğŸ”¤ Characters: {characters}
ğŸ“Š Count: {len(characters)}

ğŸŒŸ May your day be filled with joy and creativity! ğŸŒŸ
âœ¨ Wishing you success in all your endeavors! âœ¨
ğŸ’« Sending you positive energy and warm wishes! ğŸ’«

---
ğŸ’ *Even in technical difficulties, we send you our best wishes!* ğŸ’"""

@app.get("/")
async def root():
    """æ ¹è·¯å¾‘ - æœå‹™ç‹€æ…‹æª¢æŸ¥"""
    return {
        "service": "SLM NPU Font Generator",
        "status": "running",
        "timestamp": time.time(),
        "endpoints": {
            "health": "/health",
            "status": "/status",
            "generate": "/generate",
            "batch_generate": "/batch-generate",
            "chat": "/slm-chat",
            "cleanup": "/cleanup"
        }
    }

@app.post("/generate")
async def slm_generate_font(
    character: str = Form(...),
    reference_image: UploadFile = File(...),
    sampling_steps: int = Form(20),
    style_strength: float = Form(0.8)
):
    """
    ä½¿ç”¨SLM NPUç”Ÿæˆå­—å‹
    
    Args:
        character: è¦ç”Ÿæˆçš„å­—å…ƒ
        reference_image: åƒè€ƒé¢¨æ ¼åœ–ç‰‡
        sampling_steps: æ¡æ¨£æ­¥æ•¸ (1-50)
        style_strength: é¢¨æ ¼å¼·åº¦ (0.0-1.0)
    """
    try:
        print(f"[SLM] é–‹å§‹ç”Ÿæˆå­—å‹: {character}")
        print(f"[SLM] åƒæ•¸: steps={sampling_steps}, strength={style_strength}")
        
        # é©—è­‰åƒæ•¸
        if not character or len(character) != 1:
            raise HTTPException(status_code=400, detail="å­—å…ƒå¿…é ˆæ˜¯å–®å€‹å­—ç¬¦")
        
        if sampling_steps < 1 or sampling_steps > 50:
            raise HTTPException(status_code=400, detail="æ¡æ¨£æ­¥æ•¸å¿…é ˆåœ¨1-50ä¹‹é–“")
        
        if style_strength < 0.0 or style_strength > 1.0:
            raise HTTPException(status_code=400, detail="é¢¨æ ¼å¼·åº¦å¿…é ˆåœ¨0.0-1.0ä¹‹é–“")
        
        # è®€å–ä¸¦è™•ç†åœ–ç‰‡
        image_data = await reference_image.read()
        image = Image.open(io.BytesIO(image_data))
        
        # æª¢æŸ¥åœ–ç‰‡æ ¼å¼
        if image.mode == 'RGBA':
            print("[SLM] åµæ¸¬åˆ°RGBAï¼Œè½‰æ›æˆRGB")
            image = image.convert('RGB')
        
        print(f"[SLM] ä¸Šå‚³åœ–ç‰‡å¤§å°: {image.size}, æ¨¡å¼: {image.mode}")
        
        # ç²å–SLMç”Ÿæˆå™¨
        generator = get_slm_generator()
        
        # ç”Ÿæˆå­—å‹
        start_time = time.time()
        result_img = generator.generate_font(
            character=character,
            reference_image=image,
            sampling_steps=sampling_steps,
            style_strength=style_strength
        )
        generation_time = (time.time() - start_time) * 1000
        
        if result_img is None:
            raise HTTPException(status_code=500, detail="å­—å‹ç”Ÿæˆå¤±æ•—")
        
        # è½‰æ›ç‚ºbase64
        buf = io.BytesIO()
        result_img.save(buf, format="PNG")
        base64_img = base64.b64encode(buf.getvalue()).decode()
        
        print(f"[SLM] âœ… å­—å‹ç”Ÿæˆå®Œæˆ: {character}")
        print(f"[SLM] ç”Ÿæˆæ™‚é–“: {generation_time:.2f}ms")
        print(f"[SLM] Base64é è¦½: {base64_img[:50]}...")
        
        return {
            "success": True,
            "character": character,
            "image": f"data:image/png;base64,{base64_img}",
            "generation_time_ms": round(generation_time, 2),
            "model_info": generator.get_model_info()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"[SLM] âŒ å­—å‹ç”ŸæˆéŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail=f"å­—å‹ç”Ÿæˆå¤±æ•—: {str(e)}")

@app.post("/batch-generate")
async def slm_batch_generate_fonts(
    characters: str = Form(...),
    reference_image: UploadFile = File(...),
    sampling_steps: int = Form(20),
    style_strength: float = Form(0.8)
):
    """
    æ‰¹é‡ç”Ÿæˆå­—å‹
    
    Args:
        characters: è¦ç”Ÿæˆçš„å­—å…ƒå­—ç¬¦ä¸²
        reference_image: åƒè€ƒé¢¨æ ¼åœ–ç‰‡
        sampling_steps: æ¡æ¨£æ­¥æ•¸
        style_strength: é¢¨æ ¼å¼·åº¦
    """
    try:
        if not characters:
            raise HTTPException(status_code=400, detail="å­—å…ƒä¸èƒ½ç‚ºç©º")
        
        # å»é‡ä¸¦éæ¿¾
        char_list = list(set(characters.strip()))
        char_list = [c for c in char_list if c.strip()]
        
        if len(char_list) > 20:
            raise HTTPException(status_code=400, detail="ä¸€æ¬¡æœ€å¤šç”Ÿæˆ20å€‹å­—å…ƒ")
        
        print(f"[SLM] é–‹å§‹æ‰¹é‡ç”Ÿæˆå­—å‹: {char_list}")
        
        # è®€å–åœ–ç‰‡
        image_data = await reference_image.read()
        image = Image.open(io.BytesIO(image_data))
        
        if image.mode == 'RGBA':
            image = image.convert('RGB')
        
        # ç²å–ç”Ÿæˆå™¨
        generator = get_slm_generator()
        
        # æ‰¹é‡ç”Ÿæˆ
        start_time = time.time()
        results = generator.batch_generate_fonts(
            characters=char_list,
            reference_image=image,
            sampling_steps=sampling_steps,
            style_strength=style_strength
        )
        total_time = (time.time() - start_time) * 1000
        
        # è½‰æ›çµæœ
        font_images = {}
        for char, img in results.items():
            buf = io.BytesIO()
            img.save(buf, format="PNG")
            base64_img = base64.b64encode(buf.getvalue()).decode()
            font_images[char] = f"data:image/png;base64,{base64_img}"
        
        print(f"[SLM] âœ… æ‰¹é‡ç”Ÿæˆå®Œæˆ: {len(results)}/{len(char_list)} æˆåŠŸ")
        print(f"[SLM] ç¸½è€—æ™‚: {total_time:.2f}ms")
        
        return {
            "success": True,
            "total_characters": len(char_list),
            "successful_characters": len(results),
            "failed_characters": list(set(char_list) - set(results.keys())),
            "font_images": font_images,
            "total_time_ms": round(total_time, 2),
            "model_info": generator.get_model_info()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"[SLM] âŒ æ‰¹é‡ç”ŸæˆéŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡ç”Ÿæˆå¤±æ•—: {str(e)}")

@app.get("/status")
async def get_slm_status():
    """ç²å–SLMç”Ÿæˆå™¨ç‹€æ…‹"""
    try:
        generator = get_slm_generator()
        status = generator.get_model_info()
        
        return {
            "success": True,
            "status": status,
            "timestamp": time.time()
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "timestamp": time.time()
        }

@app.post("/cleanup")
async def cleanup_slm():
    """æ¸…ç†SLMç”Ÿæˆå™¨è³‡æº"""
    try:
        global slm_generator
        if slm_generator:
            slm_generator.cleanup()
            slm_generator = None
        
        return {
            "success": True,
            "message": "SLMç”Ÿæˆå™¨å·²æ¸…ç†"
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@app.post("/slm-chat")
async def slm_chat(
    message: str = Form(None, alias="user_message"),
    characters: str = Form(None),
    context: str = Form("")
):
    """
    SLM AI å°è©±ç«¯é»
    
    Args:
        message: ç”¨æˆ¶è¨Šæ¯
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é¸ï¼‰
    """
    try:
        # è™•ç†å‰ç«¯ç™¼é€çš„å­—æ®µ
        actual_message = message or characters or "è«‹åˆ†æå­—å…ƒç‰¹å¾µ"
        print(f"[SLM] ğŸ’¬ æ”¶åˆ°å°è©±è«‹æ±‚: {actual_message}")
        print(f"[SLM] ğŸ“ ä¸Šä¸‹æ–‡: {context}")
        
        # ä½¿ç”¨çœŸæ­£çš„SLMåŠŸèƒ½ç”Ÿæˆå›æ‡‰
        ai_response = _generate_slm_response(actual_message, context)
        
        print(f"[SLM] ğŸ¤– AIå›æ‡‰: {ai_response[:100]}...")
        
        return {
            "success": True,
            "user_message": actual_message,
            "slm_response": ai_response,
            "context": context,
            "timestamp": time.time()
        }
        
    except Exception as e:
        print(f"[SLM] âŒ å°è©±ç”ŸæˆéŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail=f"å°è©±ç”Ÿæˆå¤±æ•—: {str(e)}")

@app.get("/health")
async def slm_health_check():
    """SLMå¥åº·æª¢æŸ¥"""
    try:
        generator = get_slm_generator()
        status = generator.get_model_info()
        
        is_healthy = status.get("status") in ["active", "qnn_htp_active"]
        
        return {
            "service": "SLM NPU Font Generator",
            "status": "healthy" if is_healthy else "degraded",
            "model_status": status.get("status", "unknown"),
            "timestamp": time.time()
        }
        
    except Exception as e:
        return {
            "service": "SLM NPU Font Generator",
            "status": "unhealthy",
            "error": str(e),
            "timestamp": time.time()
        }

if __name__ == "__main__":
    print("ğŸš€ å•Ÿå‹•SLM NPUç¨ç«‹å¾Œç«¯æœå‹™...")
    print("ğŸ“ æœå‹™åœ°å€: http://localhost:8001")
    print("ğŸ”§ å¥åº·æª¢æŸ¥: http://localhost:8001/health")
    print("ğŸ“š APIæ–‡æª”: http://localhost:8001/docs")
    
    # å•Ÿå‹•æœå‹™å™¨ï¼Œä½¿ç”¨8001ç«¯å£é¿å…èˆ‡ç¾æœ‰å¾Œç«¯è¡çª
    uvicorn.run(
        "slm_server:app",
        host="0.0.0.0",
        port=8001,
        reload=True,
        log_level="info"
    )
