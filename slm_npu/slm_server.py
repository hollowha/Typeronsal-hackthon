"""
SLM NPU ç¨ç«‹å¾Œç«¯æœå‹™å™¨
å°ˆé–€ç”¨æ–¼SLMå­—å‹ç”Ÿæˆï¼Œä¸ä¾è³´æ–¼ç¾æœ‰çš„å¾Œç«¯
"""

from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
import io
import base64
import sys
import os
import time
from typing import Optional
import uvicorn

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from slm_generator import create_slm_generator, SLMFontGenerator

# å‰µå»ºç¨ç«‹çš„FastAPIæ‡‰ç”¨
app = FastAPI(
    title="SLM NPU Font Generator",
    description="ç¨ç«‹çš„SLM NPUå­—å‹ç”Ÿæˆæœå‹™",
    version="1.0.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # é–‹ç™¼æ™‚å…è¨±å…¨éƒ¨ä¾†æº
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€SLMç”Ÿæˆå™¨å¯¦ä¾‹
slm_generator: Optional[SLMFontGenerator] = None

def get_slm_generator() -> SLMFontGenerator:
    """ç²å–æˆ–å‰µå»ºSLMç”Ÿæˆå™¨å¯¦ä¾‹"""
    global slm_generator
    if slm_generator is None:
        # æª¢æŸ¥æ˜¯å¦æœ‰çœŸå¯¦çš„SLMæ¨¡å‹
        model_path = "./models/slm_font_model.onnx"
        use_npu = os.path.exists(model_path)
        
        slm_generator = create_slm_generator(
            model_path=model_path if use_npu else None,
            use_npu=use_npu
        )
        
        print(f"ğŸ”§ SLMç”Ÿæˆå™¨å·²åˆå§‹åŒ–: {'NPUæ¨¡å¼' if use_npu else 'æ¨¡æ“¬æ¨¡å¼'}")
    
    return slm_generator

def _generate_slm_response(characters: str, context: str) -> str:
    """ç”ŸæˆSLMå›æ‡‰ï¼ˆä½¿ç”¨çœŸæ­£çš„èªè¨€æ¨¡å‹ï¼‰"""
    try:
        # å˜—è©¦ä½¿ç”¨çœŸæ­£çš„èªè¨€æ¨¡å‹
        try:
            from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
            import torch
            
            # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„èªè¨€æ¨¡å‹
            model_name = "microsoft/DialoGPT-medium"  # ä½¿ç”¨é–‹æºçš„å°è©±æ¨¡å‹
            
            print(f"[SLM] ğŸ¤– å˜—è©¦è¼‰å…¥èªè¨€æ¨¡å‹: {model_name}")
            
            # è¼‰å…¥æ¨¡å‹å’Œtokenizer
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(model_name)
            
            # è¨­ç½®pad_token - ä¿®å¾©è­¦å‘Š
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                print(f"[SLM] ğŸ”§ è¨­ç½®pad_token: {tokenizer.pad_token}")
            
            # æ§‹å»ºç°¡å–®æœ‰æ•ˆçš„æç¤ºè©
            if context and context.strip():
                prompt = f"User: {characters} (context: {context})\nAssistant:"
            else:
                prompt = f"User: {characters}\nAssistant:"
            
            print(f"[SLM] ğŸ“ ä½¿ç”¨æç¤ºè©: {prompt}")
            
            # ç·¨ç¢¼è¼¸å…¥
            inputs = tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=50)
            
            # ç”Ÿæˆå›æ‡‰ - ä½¿ç”¨æ›´ä¿å®ˆçš„åƒæ•¸
            with torch.no_grad():
                outputs = model.generate(
                    inputs, 
                    max_new_tokens=80,
                    num_return_sequences=1,
                    temperature=0.9,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=2
                )
            
            # è§£ç¢¼å›æ‡‰
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰åŸå§‹æç¤ºï¼‰
            generated_response = response[len(prompt):].strip()
            
            if generated_response:
                print(f"[SLM] âœ… èªè¨€æ¨¡å‹ç”ŸæˆæˆåŠŸ")
                return generated_response
            else:
                print(f"[SLM] âš ï¸ èªè¨€æ¨¡å‹å›æ‡‰ç‚ºç©ºï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ")
                raise Exception("æ¨¡å‹å›æ‡‰ç‚ºç©º")
                
        except Exception as model_error:
            print(f"[SLM] âš ï¸ èªè¨€æ¨¡å‹è¼‰å…¥å¤±æ•—: {model_error}")
            print(f"[SLM] ğŸ”„ åˆ‡æ›åˆ°æ™ºèƒ½æ¨¡æ¿æ¨¡å¼")
            
            # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ›´æ™ºèƒ½çš„æ¨¡æ¿ç”Ÿæˆ
            return _generate_smart_template_response(characters, context)
            
    except Exception as e:
        print(f"[SLM] âŒ ç”ŸæˆSLMå›æ‡‰æ™‚å‡ºéŒ¯: {e}")
        return f"SLMå›æ‡‰ç”Ÿæˆå¤±æ•—: {str(e)}"

def _generate_smart_template_response(characters: str, context: str) -> str:
    """æ™ºèƒ½æ¨¡æ¿å›æ‡‰ï¼ˆç•¶èªè¨€æ¨¡å‹ä¸å¯ç”¨æ™‚ï¼‰"""
    try:
        # æ ¹æ“šå­—å…ƒç‰¹å¾µç”Ÿæˆæ›´æ™ºèƒ½çš„å›æ‡‰
        char_count = len(characters)
        
        # åˆ†æå­—å…ƒé¡å‹
        has_chinese = any('\u4e00' <= char <= '\u9fff' for char in characters)
        has_english = any(char.isascii() and char.isalpha() for char in characters)
        has_numbers = any(char.isdigit() for char in characters)
        
        # æ ¹æ“šå­—å…ƒé¡å‹é¸æ“‡å›æ‡‰ç­–ç•¥
        if has_chinese:
            if char_count <= 3:
                analysis = f"é€™{char_count}å€‹ä¸­æ–‡å­—å…ƒçµæ§‹ç›¸å°ç°¡å–®ï¼Œç­†ç•«æ¸…æ™°ï¼Œé©åˆå¿«é€Ÿç”Ÿæˆã€‚"
            elif char_count <= 8:
                analysis = f"é€™{char_count}å€‹ä¸­æ–‡å­—å…ƒåŒ…å«å¤šç¨®ç­†ç•«çµæ§‹ï¼Œéœ€è¦å¹³è¡¡ç”Ÿæˆè³ªé‡å’Œé€Ÿåº¦ã€‚"
            else:
                analysis = f"é€™{char_count}å€‹ä¸­æ–‡å­—å…ƒæ•¸é‡è¼ƒå¤šï¼Œå»ºè­°åˆ†æ‰¹è™•ç†ä»¥ç²å¾—æœ€ä½³æ•ˆæœã€‚"
        elif has_english:
            analysis = f"é€™{char_count}å€‹è‹±æ–‡å­—æ¯çµæ§‹è¦å¾‹ï¼Œç”Ÿæˆé€Ÿåº¦æœƒè¼ƒå¿«ã€‚"
        elif has_numbers:
            analysis = f"é€™{char_count}å€‹æ•¸å­—å­—å…ƒçµæ§‹ç°¡å–®ï¼Œç”Ÿæˆæ•ˆç‡æœ€é«˜ã€‚"
        else:
            analysis = f"é€™{char_count}å€‹æ··åˆå­—å…ƒéœ€è¦ç¶œåˆè€ƒæ…®å„ç¨®å› ç´ ã€‚"
        
        # æ ¹æ“šä¸Šä¸‹æ–‡ç”Ÿæˆå»ºè­°
        if "åˆ†æ" in context or "ç‰¹å¾µ" in context:
            suggestion = "å»ºè­°ä½¿ç”¨æ·±åº¦å­¸ç¿’æ¨¡å‹é€²è¡Œç­†ç•«åˆ†æå’Œé¢¨æ ¼åŒ¹é…ï¼Œç¢ºä¿ç”Ÿæˆçš„å­—å‹ä¿æŒè¦–è¦ºä¸€è‡´æ€§ã€‚"
        elif "ç”Ÿæˆ" in context:
            suggestion = "å­—å‹ç”Ÿæˆå°‡ä½¿ç”¨ç¥ç¶“ç¶²çµ¡é€²è¡Œé¢¨æ ¼é·ç§»ï¼Œæ¯å€‹å­—å…ƒéƒ½æœƒç¶“éå„ªåŒ–è™•ç†ã€‚"
        else:
            suggestion = "å­—å‹ç”Ÿæˆéç¨‹æœƒè€ƒæ…®å­—å…ƒé–“çš„è¦–è¦ºå”èª¿æ€§ï¼Œæœ€çµ‚è¼¸å‡ºå°‡ä¿æŒæ‰‹å¯«é¢¨æ ¼çš„è‡ªç„¶æ€§ã€‚"
        
        # æŠ€è¡“ç´°ç¯€
        technical_details = [
            "ä½¿ç”¨å·ç©ç¥ç¶“ç¶²çµ¡é€²è¡Œå­—å…ƒç‰¹å¾µæå–",
            "æ¡ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶ç¢ºä¿ç­†ç•«çš„é€£çºŒæ€§",
            "é€šéå°æŠ—è¨“ç·´æå‡å­—å‹çš„çœŸå¯¦æ„Ÿ",
            "ä½¿ç”¨é¢¨æ ¼é·ç§»æŠ€è¡“ä¿æŒåƒè€ƒåœ–ç‰‡çš„é¢¨æ ¼"
        ]
        
        import random
        selected_details = random.sample(technical_details, 2)
        
        # æ§‹å»ºå®Œæ•´å›æ‡‰
        response = f"""å­—å…ƒ '{characters}' çš„æ™ºèƒ½åˆ†æï¼š

{analysis}

{suggestion}

æŠ€è¡“å¯¦ç¾ï¼š
{chr(10).join(f"â€¢ {detail}" for detail in selected_details)}

ç”Ÿæˆå»ºè­°ï¼šæ ¹æ“šå­—å…ƒè¤‡é›œåº¦ï¼Œé è¨ˆç”Ÿæˆæ™‚é–“ç´„ {char_count * 2} ç§’ã€‚"""
        
        return response
        
    except Exception as e:
        print(f"[SLM] âŒ æ™ºèƒ½æ¨¡æ¿ç”Ÿæˆå¤±æ•—: {e}")
        return f"æ™ºèƒ½åˆ†æå¤±æ•—: {str(e)}"

@app.get("/")
async def root():
    """æ ¹è·¯å¾‘ - æœå‹™ç‹€æ…‹æª¢æŸ¥"""
    return {
        "service": "SLM NPU Font Generator",
        "status": "running",
        "timestamp": time.time(),
        "endpoints": {
            "health": "/health",
            "status": "/status",
            "generate": "/generate",
            "batch_generate": "/batch-generate",
            "chat": "/slm-chat",
            "cleanup": "/cleanup"
        }
    }

@app.post("/generate")
async def slm_generate_font(
    character: str = Form(...),
    reference_image: UploadFile = File(...),
    sampling_steps: int = Form(20),
    style_strength: float = Form(0.8)
):
    """
    ä½¿ç”¨SLM NPUç”Ÿæˆå­—å‹
    
    Args:
        character: è¦ç”Ÿæˆçš„å­—å…ƒ
        reference_image: åƒè€ƒé¢¨æ ¼åœ–ç‰‡
        sampling_steps: æ¡æ¨£æ­¥æ•¸ (1-50)
        style_strength: é¢¨æ ¼å¼·åº¦ (0.0-1.0)
    """
    try:
        print(f"[SLM] é–‹å§‹ç”Ÿæˆå­—å‹: {character}")
        print(f"[SLM] åƒæ•¸: steps={sampling_steps}, strength={style_strength}")
        
        # é©—è­‰åƒæ•¸
        if not character or len(character) != 1:
            raise HTTPException(status_code=400, detail="å­—å…ƒå¿…é ˆæ˜¯å–®å€‹å­—ç¬¦")
        
        if sampling_steps < 1 or sampling_steps > 50:
            raise HTTPException(status_code=400, detail="æ¡æ¨£æ­¥æ•¸å¿…é ˆåœ¨1-50ä¹‹é–“")
        
        if style_strength < 0.0 or style_strength > 1.0:
            raise HTTPException(status_code=400, detail="é¢¨æ ¼å¼·åº¦å¿…é ˆåœ¨0.0-1.0ä¹‹é–“")
        
        # è®€å–ä¸¦è™•ç†åœ–ç‰‡
        image_data = await reference_image.read()
        image = Image.open(io.BytesIO(image_data))
        
        # æª¢æŸ¥åœ–ç‰‡æ ¼å¼
        if image.mode == 'RGBA':
            print("[SLM] åµæ¸¬åˆ°RGBAï¼Œè½‰æ›æˆRGB")
            image = image.convert('RGB')
        
        print(f"[SLM] ä¸Šå‚³åœ–ç‰‡å¤§å°: {image.size}, æ¨¡å¼: {image.mode}")
        
        # ç²å–SLMç”Ÿæˆå™¨
        generator = get_slm_generator()
        
        # ç”Ÿæˆå­—å‹
        start_time = time.time()
        result_img = generator.generate_font(
            character=character,
            reference_image=image,
            sampling_steps=sampling_steps,
            style_strength=style_strength
        )
        generation_time = (time.time() - start_time) * 1000
        
        if result_img is None:
            raise HTTPException(status_code=500, detail="å­—å‹ç”Ÿæˆå¤±æ•—")
        
        # è½‰æ›ç‚ºbase64
        buf = io.BytesIO()
        result_img.save(buf, format="PNG")
        base64_img = base64.b64encode(buf.getvalue()).decode()
        
        print(f"[SLM] âœ… å­—å‹ç”Ÿæˆå®Œæˆ: {character}")
        print(f"[SLM] ç”Ÿæˆæ™‚é–“: {generation_time:.2f}ms")
        print(f"[SLM] Base64é è¦½: {base64_img[:50]}...")
        
        return {
            "success": True,
            "character": character,
            "image": f"data:image/png;base64,{base64_img}",
            "generation_time_ms": round(generation_time, 2),
            "model_info": generator.get_model_info()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"[SLM] âŒ å­—å‹ç”ŸæˆéŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail=f"å­—å‹ç”Ÿæˆå¤±æ•—: {str(e)}")

@app.post("/batch-generate")
async def slm_batch_generate_fonts(
    characters: str = Form(...),
    reference_image: UploadFile = File(...),
    sampling_steps: int = Form(20),
    style_strength: float = Form(0.8)
):
    """
    æ‰¹é‡ç”Ÿæˆå­—å‹
    
    Args:
        characters: è¦ç”Ÿæˆçš„å­—å…ƒå­—ç¬¦ä¸²
        reference_image: åƒè€ƒé¢¨æ ¼åœ–ç‰‡
        sampling_steps: æ¡æ¨£æ­¥æ•¸
        style_strength: é¢¨æ ¼å¼·åº¦
    """
    try:
        if not characters:
            raise HTTPException(status_code=400, detail="å­—å…ƒä¸èƒ½ç‚ºç©º")
        
        # å»é‡ä¸¦éæ¿¾
        char_list = list(set(characters.strip()))
        char_list = [c for c in char_list if c.strip()]
        
        if len(char_list) > 20:
            raise HTTPException(status_code=400, detail="ä¸€æ¬¡æœ€å¤šç”Ÿæˆ20å€‹å­—å…ƒ")
        
        print(f"[SLM] é–‹å§‹æ‰¹é‡ç”Ÿæˆå­—å‹: {char_list}")
        
        # è®€å–åœ–ç‰‡
        image_data = await reference_image.read()
        image = Image.open(io.BytesIO(image_data))
        
        if image.mode == 'RGBA':
            image = image.convert('RGB')
        
        # ç²å–ç”Ÿæˆå™¨
        generator = get_slm_generator()
        
        # æ‰¹é‡ç”Ÿæˆ
        start_time = time.time()
        results = generator.batch_generate_fonts(
            characters=char_list,
            reference_image=image,
            sampling_steps=sampling_steps,
            style_strength=style_strength
        )
        total_time = (time.time() - start_time) * 1000
        
        # è½‰æ›çµæœ
        font_images = {}
        for char, img in results.items():
            buf = io.BytesIO()
            img.save(buf, format="PNG")
            base64_img = base64.b64encode(buf.getvalue()).decode()
            font_images[char] = f"data:image/png;base64,{base64_img}"
        
        print(f"[SLM] âœ… æ‰¹é‡ç”Ÿæˆå®Œæˆ: {len(results)}/{len(char_list)} æˆåŠŸ")
        print(f"[SLM] ç¸½è€—æ™‚: {total_time:.2f}ms")
        
        return {
            "success": True,
            "total_characters": len(char_list),
            "successful_characters": len(results),
            "failed_characters": list(set(char_list) - set(results.keys())),
            "font_images": font_images,
            "total_time_ms": round(total_time, 2),
            "model_info": generator.get_model_info()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"[SLM] âŒ æ‰¹é‡ç”ŸæˆéŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡ç”Ÿæˆå¤±æ•—: {str(e)}")

@app.get("/status")
async def get_slm_status():
    """ç²å–SLMç”Ÿæˆå™¨ç‹€æ…‹"""
    try:
        generator = get_slm_generator()
        status = generator.get_model_info()
        
        return {
            "success": True,
            "status": status,
            "timestamp": time.time()
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "timestamp": time.time()
        }

@app.post("/cleanup")
async def cleanup_slm():
    """æ¸…ç†SLMç”Ÿæˆå™¨è³‡æº"""
    try:
        global slm_generator
        if slm_generator:
            slm_generator.cleanup()
            slm_generator = None
        
        return {
            "success": True,
            "message": "SLMç”Ÿæˆå™¨å·²æ¸…ç†"
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@app.post("/slm-chat")
async def slm_chat(
    message: str = Form(None, alias="user_message"),
    characters: str = Form(None),
    context: str = Form("")
):
    """
    SLM AI å°è©±ç«¯é»
    
    Args:
        message: ç”¨æˆ¶è¨Šæ¯
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é¸ï¼‰
    """
    try:
        # è™•ç†å‰ç«¯ç™¼é€çš„å­—æ®µ
        actual_message = message or characters or "è«‹åˆ†æå­—å…ƒç‰¹å¾µ"
        print(f"[SLM] ğŸ’¬ æ”¶åˆ°å°è©±è«‹æ±‚: {actual_message}")
        print(f"[SLM] ğŸ“ ä¸Šä¸‹æ–‡: {context}")
        
        # ä½¿ç”¨çœŸæ­£çš„SLMåŠŸèƒ½ç”Ÿæˆå›æ‡‰
        ai_response = _generate_slm_response(actual_message, context)
        
        print(f"[SLM] ğŸ¤– AIå›æ‡‰: {ai_response[:100]}...")
        
        return {
            "success": True,
            "user_message": actual_message,
            "slm_response": ai_response,
            "context": context,
            "timestamp": time.time()
        }
        
    except Exception as e:
        print(f"[SLM] âŒ å°è©±ç”ŸæˆéŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail=f"å°è©±ç”Ÿæˆå¤±æ•—: {str(e)}")

@app.get("/health")
async def slm_health_check():
    """SLMå¥åº·æª¢æŸ¥"""
    try:
        generator = get_slm_generator()
        status = generator.get_model_info()
        
        is_healthy = status.get("status") in ["active", "qnn_htp_active"]
        
        return {
            "service": "SLM NPU Font Generator",
            "status": "healthy" if is_healthy else "degraded",
            "model_status": status.get("status", "unknown"),
            "timestamp": time.time()
        }
        
    except Exception as e:
        return {
            "service": "SLM NPU Font Generator",
            "status": "unhealthy",
            "error": str(e),
            "timestamp": time.time()
        }

if __name__ == "__main__":
    print("ğŸš€ å•Ÿå‹•SLM NPUç¨ç«‹å¾Œç«¯æœå‹™...")
    print("ğŸ“ æœå‹™åœ°å€: http://localhost:8001")
    print("ğŸ”§ å¥åº·æª¢æŸ¥: http://localhost:8001/health")
    print("ğŸ“š APIæ–‡æª”: http://localhost:8001/docs")
    
    # å•Ÿå‹•æœå‹™å™¨ï¼Œä½¿ç”¨8001ç«¯å£é¿å…èˆ‡ç¾æœ‰å¾Œç«¯è¡çª
    uvicorn.run(
        "slm_server:app",
        host="0.0.0.0",
        port=8001,
        reload=True,
        log_level="info"
    )
