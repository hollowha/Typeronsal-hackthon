#!/usr/bin/env python3
"""
æ¸¬è©¦ Phi-2 æ¨¡å‹è¼‰å…¥
"""

def test_phi2():
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        print("ğŸ”„ æ¸¬è©¦ Phi-2 æ¨¡å‹è¼‰å…¥...")
        
        # è¼‰å…¥ tokenizer
        print("ğŸ“¥ è¼‰å…¥ tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2")
        
        # è¨­ç½® pad_token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            print("âœ… è¨­ç½® pad_token")
        
        # è¼‰å…¥æ¨¡å‹
        print("ğŸ“¥ è¼‰å…¥æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained("microsoft/phi-2")
        
        # æ¸¬è©¦ç”Ÿæˆ
        print("ğŸ§ª æ¸¬è©¦ç”Ÿæˆ...")
        prompt = "User: è«‹åˆ†æã€Œä½ å¥½ã€ä¸¦æä¾›å»ºè­°ã€‚\n\nAssistant:"
        inputs = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=50,
                temperature=0.7,
                do_sample=True,
                repetition_penalty=1.1,
                top_k=50,
                top_p=0.9
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ… æ¸¬è©¦æˆåŠŸï¼")
        print(f"ğŸ“ å›æ‡‰: {response}")
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")

if __name__ == "__main__":
    test_phi2()
